# Project Data Warehouse
### Overview 
Data Modeling for "Sparkify" an online music streaming platform. The startup was collecting data on songs and user activity on the streaming platform.They have grown their user database and song database and now they want to move their processes and data onto the cloud AWS. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.In this project, using ETL pipeline we will  create a data warehouses hosted on Redshift which can be further used by the analytics team for getting tons of insights from the available json format data.

## Song Dataset 
We will be working with two datasets Song dataset and Log dataset that reside in S3. Their desciption is provided below :

#### Song Dataset: 
It is a subset of real data from [Million Song Dataset]. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

#### Log Dataset:
This dataset consists of log files in JSON format generated by this  [event simulator] based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.The log files in the dataset are partitioned by year and month. 

## Schema for Song Play Analysis

#### Fact Table
##### songplays
    records in event data associated with song plays. Columns for the table:
    songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimension Tables 
##### users
    user_id, first_name, last_name, gender, level
##### songs
    song_id, title, artist_id, year, duration
##### artists
    artist_id, name, location, lattitude, longitude
##### time
    start_time, hour, day, week, month, year, weekday

#### Project files
There are 5 files in this project.
1. dwh.cfg - This file conatins AWS credentials,redshift credentials and IAM role info.
2. sql_queries.py - Script which contains the drop and create tables along with staging tables.It also contains insert statements along with query lists.
3. create_tables.py - Program which contains queries for dropping and creating the database and tables.
4. etl.py - Script which contains the complete ETL pipeline for the project.
5. Readme.md - Documentation regarding the project.


### To Run the project
1. To run the project, all the above files should be in a single Directory.
2. Setup the dwh.cfg file with required credentials.Add S3 links,redshift database and IAM role info to dwh.cfg
3. After designing schemas for fact and dimension tables,write SQL CREATE statements for each of these tables in sql_queries.py.
4. Use create_tables.py to connect to the database and create these tables.
5. Launch a redshift cluster and create an IAM role that has read access to S3.
6. Test by running create_tables.py in terminal by typing "python create_tables.py" and checking the table schemas in your redshift database. We can also use  Query Editor in the AWS Redshift console for this.
7. etl.py scipt is used to load data from S3 to staging tables on Redshift and to load data from staging tables to analytics tables on Redshift
8. Test by running etl.py by typing "python etl.py" in terminal after running create_tables.py and running the analytic queries on Redshift database to compare results with the expected results.
9. Delete redshift cluster when finished.




